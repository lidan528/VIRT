export CUDA_VISIBLE_DEVICES=3
export BERT_BASE_DIR=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_mini128_L6h384_comment128_initwithstu
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_mini128_L6h384I1200_comment128/e3loss0.1287
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_minilm_L3h384I1200_comment128
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/robertalarge_baikexw_alldata_continue/ckpt0.6859
#BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/robertaxxxlarge_alldata_5e7warm1e/ckpt0.7043
BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/robertalarge_baikexw_alldata_continue_xlarge/ckpt0.71
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/student_3_384_comment_initfromcommon
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_mini128_L6h384I1200_comment512/ckptloss0.1204e0.3
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_minilm_L3h384I1200_comment128_wa
#export BERT_MODEL=/home/hadoop-aipnlp/cephfs/data/transfer_to_zw03/yangyang113/bert_base/train_log/bert_distil_minilm_L6h128I512A2_common128_wa
export MY_DATASET=./train_data/ #全局变量 数据集所在地址
export OUTPUT=./output/robertaxlarge_5e6_seq128bs32e3/
python run_classifier.py \
  --task_name=lcqmc \
  --do_train=true \
  --do_eval=true \
  --do_predict=true \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config_xlarge.json \
  --init_checkpoint=$BERT_MODEL/model.ckpt-73002 \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --predict_batch_size=128 \
  --learning_rate=5e-6 \
  --num_train_epochs=3.0 \
  --output_dir=$OUTPUT \
