#### 数据&模型参数
包括MNLI、RTE、QQP、BOOLQ。以上均为GLUE/SuperGLUE的标准版本。

```
${root_dir}$/datasets
├── boolq
│   ├── train.jsonl
│   └── val.jsonl
├── mnli
│   ├── dev.jsonl
│   └── train.jsonl
├── qqp
│   ├── dev.jsonl
│   └── train.jsonl
|── rte
│   ├── dev.jsonl
│   └── train.jsonl
```
BERT词表为默认词表： [link](https://github.com/StonyBrookNLP/deformer/releases/download/v1.0/bert.vocab)

BERT的预训练参数为BERT-base的uncased版本： [link](https://github.com/google-research/bert)

#### 训练与评测

- 首先在各数据集上finetune BERT-base模型作为teacher。注意输入的`<PAD>`位置与标准位置稍有不同，为`<CLS>, token,...,token, <PAD>,..., <PAD>, <SEP> token,...,token,<PAD>,..., <PAD>,<SEP>`格式。输出采用所有有效`token`的Embedding的`mean_pooling`。
- 在 MNLI/QQP 数据集上训练BERT-base: (对应代码库中的`2.bert_bipartition.sh`)

```shell
export BERT_BASE_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/english_bert_base_model/uncased_L-12_H-768_A-12
export MY_DATASET=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/dataset/mnli/
export OUTPUT=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/mnli/bert_base_bipartition_mean_pool/
python run_classifier_bipartition.py \
  --task_name=mnli \
  --do_train=true \
  --do_eval=true \
  --do_predict=false \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=259 \
  --train_batch_size=40 \
  --learning_rate=5e-5 \
  --num_train_epochs=6.0 \
  --output_dir=$OUTPUT \
  --pooling_strategy=mean
```

- 在 RTE/BOOLQ 数据集上训练BERT-base: (对应代码库中的`_--2.bert_bipartition_boolq`)

```shell
export BERT_BASE_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/english_bert_base_model/uncased_L-12_H-768_A-12
export MY_DATASET=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/dataset/boolq/
export OUTPUT=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/boolq/bert_base_bipartition_mean_pool/
python _--run_classifier_boolq.py \
  --task_name=boolq \
  --do_train=true \
  --do_eval=true \
  --do_predict=false \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length_query=64 \
  --max_seq_length_doc=328 \
  --train_batch_size=28 \
  --learning_rate=5e-5 \
  --num_train_epochs=30.0 \
  --output_dir=$OUTPUT \
  --pooling_strategy=mean
```

- 采用BERT-base作为teacher模型蒸馏双塔模型。双塔模型的输入也遵循先`<PAD>`最后插入`<SEP>`规则。双塔的模型的输出也采用对有效token的Embedding做`mean_pooling`操作。
  - `--model_type`：指定双塔模型的类别
  - `--kd_weight_att`：指定注意力蒸馏的权重(经过测试为400~700较好)
  - `--use_kd_att`：是否需要使用注意力蒸馏
  - `--use_kd_logit_mse`：采用`MSE loss`蒸馏logits
  - `--use_kd_logit_kl`：采用`KL `散度蒸馏logits
  - `--use_resnet_predict`：如果`model_type`指定为`late_fusion`，则此处为`true`，否则为`false`
  - `--pooling_strategy`：设置为`mean`

- 例如在MNLI/QQP 数据集上训练VIRT-encoder：(对应`late_fusion_att700_qqp.sh`)

```shell
export CUDA_VISIBLE_DEVICES=0
export BERT_MODEL_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/english_bert_base_model/uncased_L-12_H-768_A-12
export TEACHER_MODEL_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/qqp/bert_base_bipartition_mean_pool
export MY_DATASET=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/dataset/qqp/ #全局变量 
export OUTPUT=/home/hadoop aipnlp/cephfs/data/lidan65/distill/output/qqp/late_fusion_att700/
python sbert_distill.py \
  --task_name=qqp \
  --pooling_strategy=mean \
  --do_train=true \
  --do_eval=true \
  --do_predict=false \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_MODEL_DIR/vocab.txt \
  --bert_config_file=$BERT_MODEL_DIR/bert_config.json \
  --init_checkpoint_teacher=$TEACHER_MODEL_DIR/model.ckpt-46000 \
  --init_checkpoint_student=$BERT_MODEL_DIR/bert_model.ckpt \
  --max_seq_length_bert=259 \
  --max_seq_length_sbert=130 \
  --train_batch_size=32 \
  --learning_rate=5e-5 \
  --test_flie_name=None \
  --num_train_epochs=6 \
  --do_save=false \
  --output_dir=$OUTPUT \
  --use_kd_logit_kl=false \
  --use_kd_logit_mse=false \
  --kd_weight_logit=0.4 \
  --use_kd_att=true \
  --kd_weight_att=700 \
  --use_all_layer_emb=false \ # 无效尝试，置为false
  --use_resnet_predict=true \
  --use_weighted_att=false \ # 无效尝试，置为false
  --use_att_head=false \    # 无效尝试，置为false
  --model_type=late_fusion
```

- 在 BOOLQ/RTE 数据集上训练VIRT-encoder: (对应`late_fusion_att700_rte.sh`)

  ```shell
  export BERT_MODEL_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/english_bert_base_model/uncased_L-12_H-768_A-12
  export TEACHER_MODEL_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/rte/bert_base_bipartition_mean_pool
  export MY_DATASET=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/dataset/rte/ 
  export OUTPUT=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/rte/late_fusion_att700/
  python _--sbert_distill_boolq.py \
    --task_name=rte \
    --pooling_strategy=mean \
    --do_train=true \
    --do_eval=true \
    --do_predict=false \
    --data_dir=$MY_DATASET \
    --vocab_file=$BERT_MODEL_DIR/vocab.txt \
    --bert_config_file=$BERT_MODEL_DIR/bert_config.json \
    --init_checkpoint_teacher=$TEACHER_MODEL_DIR/model.ckpt-2000 \
    --init_checkpoint_student=$BERT_MODEL_DIR/bert_model.ckpt \
    --max_seq_length_query=64 \
    --max_seq_length_doc=328 \
    --train_batch_size=28 \
    --learning_rate=5e-5 \
    --test_flie_name=None \
    --num_train_epochs=30 \
    --do_save=false \
    --output_dir=$OUTPUT \
    --use_kd_logit_kl=false \
    --use_kd_logit_mse=false \
    --kd_weight_logit=0.4 \
    --use_kd_att=true \
    --kd_weight_att=700 \
    --use_resnet_predict=true \
    --model_type=late_fusion
  ```

#### 计算FLOPs
要评测类型为`$model_type$`的模型的FLOPs，可以运行代码库中的`eval_flops_$model_type$.sh`。在对应的`eval_flops_$model_type$.py`文件中，需要注意将`main(_)`函数中的`metric_flops(bert_config)`取消注释。

例如评测VIRT-encoder的FLOPs：

```shell
export CUDA_VISIBLE_DEVICES=0
export BERT_MODEL_DIR=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/english_bert_base_model/uncased_L-12_H-768_A-12
export MY_DATASET=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/dataset/mnli/ 
export OUTPUT=/home/hadoop-aipnlp/cephfs/data/lidan65/distill/output/mnli/s_bert_base_pad_sep_realpool/
python eval_flops_virt.py \
  --task_name=boolq \
  --pooling_strategy=mean \
  --do_train=false \
  --do_eval=true \
  --do_predict=false \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_MODEL_DIR/vocab.txt \
  --bert_config_file=$BERT_MODEL_DIR/bert_config.json \
  --init_checkpoint=$BERT_MODEL_DIR/bert_model.ckpt \
  --max_seq_length_query=64 \
  --max_seq_length_doc=328 \
  --train_batch_size=1 \
  --learning_rate=5e-5 \
  --test_flie_name=None \
  --num_train_epochs=5.0 \
  --do_save=false \
  --output_dir=$OUTPUT
```

